{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 1: Scrapping the web, Nobel Price Laureates\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We are going to scrape the web to extract information about the different Nobel price laureates. This homework is designed to get you familiarized with some of the python data structures.\n",
    "\n",
    "## Getting the data\n",
    "\n",
    "We are going to get the data of all the [Nobel price laureates in Physics from Wikipedia](https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Physics). I wrote a small web parser to parse the table in a pandas dataframe. It is not important that you fully understand how it works but it does not hurt to try! I am using the [`httplib2`](https://github.com/httplib2/httplib2) and [`bs4`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) libraries. Be sure to download them:\n",
    "\n",
    ">In a terminal window type\n",
    "```\n",
    "source activate YOUR_ENVIRONMENT\n",
    "pip install httplib2\n",
    "pip install bs4\n",
    "source deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This line to ensure the use of plots within Jupyter\n",
    "%matplotlib inline\n",
    "# We import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from httplib2 import Http\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    def __init__(self, url):  \n",
    "        http = Http()\n",
    "        status, response = http.request(url)\n",
    "        tables = BeautifulSoup(response, \"lxml\", \n",
    "                              parse_only=SoupStrainer(\"table\", {\"class\":\"wikitable sortable\"}))\n",
    "        self.table = tables.contents[1]\n",
    "    \n",
    "    def parse_table(self):      \n",
    "        rows = self.table.find_all(\"tr\")\n",
    "        header = self.parse_header(rows[0])\n",
    "        table_array = [self.parse_row(row) for row in rows[1:]]\n",
    "        table_df = pd.DataFrame(table_array, columns=header).apply(self.clean_table, 1)\n",
    "        return table_df.replace({\"Year\":{'':np.nan}})\n",
    "        \n",
    "    def parse_row(self, row):     \n",
    "        columns = row.find_all(\"td\")\n",
    "        return [BeautifulSoup.get_text(col).strip() for col in columns if BeautifulSoup.get_text(col) != '']\n",
    "    \n",
    "    def parse_header(self, row):     \n",
    "        columns = row.find_all(\"th\")\n",
    "        return [BeautifulSoup.get_text(col).strip() for col in columns if BeautifulSoup.get_text(col) != \"\"]\n",
    "    \n",
    "    def clean_table(self, row):\n",
    "        if not row.iloc[0].isdigit() and row.iloc[0] != '':\n",
    "            return row.shift(1)\n",
    "        else:\n",
    "            return row\n",
    "        \n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Physics\"        \n",
    "parser = Parser(url)   \n",
    "nobel_df = parser.parse_table()\n",
    "nobel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "As you can see the data is a bit messy so we need to clean a bit. We need to:\n",
    "\n",
    ">- clean the columns names by changing them to: \"Year\", \"Laureate\", \"Country\", \"Rationale\".\n",
    "- remove the rows that where the Nobel price was not awarded (the ones with missing values). You can use the [`pd.dropna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) function with the argument `subset`.\n",
    "- fill the missing values in the year and rational columns. You can use the [`pd.fillna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) function with the argument `method='ffill'` (you can do that here because the rows are ordered by date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Clean the columns names\n",
    "\n",
    "# TODO: drop all the rows where the nobel price was not awarded\n",
    "\n",
    "# TODO: fill the missing values in the year column\"\n",
    "\n",
    "# Is your data clean?\n",
    "nobel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check that our data set does not contain missing values anymore\n",
    "nobel_df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some questions about this data\n",
    "\n",
    "Let's answer few questions about this data (with codes). \n",
    "\n",
    ">- How many physicists got a Nobel price? Be careful about possible duplicates. You can look at the [`pd.nunique()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html) function.\n",
    "- How many countries are in this data set? Be careful about possible duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How many physicists got a nobel price?\n",
    "physicist_number =  # YOUR CODE\n",
    "\n",
    "# TODO: How many countries are in this data set?\n",
    "country_number =  # YOUR CODE\n",
    "\n",
    "print(physicist_number)\n",
    "print(country_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe you have noticed that some values for the Column \"Country\" are represented by 2 countries separated by a return character (i.e. \"Austria-Hungary\\n Germany\"). Let's try to observe the distribution of countries in this data set.\n",
    "\n",
    ">- Use the [`pandas.Series.str.split`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html) function to split the column \"Country\" into a column \"Country_list\" of lists of countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: split the column \"Country\" into a column \"Country_list\" of lists of countries\n",
    "nobel_df[\"Country_list\"] =  # YOUR CODE\n",
    "\n",
    "# We create a pandas series from this new column to ease the analysis on the countries. The sum on list is used to \n",
    "# flatten the list of lists into one list of countries.\n",
    "countries = pd.Series(sum(nobel_df[\"Country_list\"].tolist(), [])).str.strip()\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of countries:\n",
    ">- Print the countries with the number of time it is contained in the `countries` pandas Series. You can use the [`pd.Series.value_counts`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) function.\n",
    "- Plot a barplot ordered by those number. You can use the function [`pd.plot`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) by changing the argument `kind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print the countries and the number of times they are contained in the countries pandas Series. It should\n",
    "# be printed ordered by the number of times they are contained in the countries pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a barplot ordered by those number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What type of physics those physicists are practicing?\n",
    "\n",
    "Let's try to gather some data to understand what type of physics is associated to each of those physicists. Ultimately we want to extract the words that are characteristics of each physicist.\n",
    "\n",
    "We extract the webpage links to have access to their bibliography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from httplib2 import Http\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "http = Http()\n",
    "status, response = http.request(url)\n",
    "\n",
    "table = BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer('table'))\n",
    "link_df = pd.DataFrame([[x.string, x[\"href\"]] for x in table.contents[1].find_all(\"a\")],\n",
    "                       columns=[\"Text\", \"link\"]).drop_duplicates()\n",
    "\n",
    "link_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to merge this table to the `nobel_df` table. Use the [`pandas.DataFrame.merge`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) or  [`pandas.concat`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) function to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: merge nobel_df and link_df into nobel_merged_df\n",
    "nobel_merged_df =  # YOUR CODE\n",
    "nobel_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Did the merging completely work? Are there some missing values? If yes correct it\n",
    "\n",
    "Now we are going to extract all the words in the Wikipedia page of each of those physicists. The following function `get_text` will extract the text of a Wikipedia page as a long string. \n",
    "\n",
    ">Use it to extract every text for each of the physicists into the columns \"Bio\". Use the function [`apply`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html) to vectorize your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Will Extract the text associated to every link\n",
    "def get_text(link, root_website = \"https://en.wikipedia.org\"):    \n",
    "    http = Http()\n",
    "    status, response = http.request(root_website + link)\n",
    "\n",
    "    body = BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer(\"div\", {\"id\":\"mw-content-text\"}))\n",
    "    return BeautifulSoup.get_text(body.contents[1])\n",
    "\n",
    "nobel_merged_df.set_index(\"Laureate\", inplace=True)\n",
    "\n",
    "# TODO: extract the text of the wikipedia page associated to each physicist\n",
    "nobel_merged_df[\"Bio\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Bio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove all the punctuation along with the number and set all the words to lower case. We import the punctuation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an example of how to remove punctuation and the numbers for one bio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in punctuation + \"1234567890\":\n",
    "    nobel_merged_df[\"Bio\"][0] = nobel_merged_df[\"Bio\"][0].replace(p,'').lower()  \n",
    "    \n",
    "nobel_merged_df[\"Bio\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Write a function and then use the pandas `apply` to treat all the bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function that remove the punctuation and numbers and set every word to lower case\n",
    "def clean_string(string):\n",
    "    # TODO: your code goes here\n",
    "    pass\n",
    "\n",
    "# TODO: apply this function to the \"Bio\" column\n",
    "nobel_merged_df[\"Bio\"] =  # YOUR CODE  \n",
    "nobel_merged_df[\"Bio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Use the `str.split` function again to split each text on any whitespace character (i.e \"\\s\") into the \"Bio_split\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split the \"Bio\" column as a column of lists of the words \n",
    "nobel_merged_df[\"Bio_list\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Bio_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are a lot of empty elements in each of those lists. We can remove those using the [`filter`](http://book.pythontips.com/en/latest/map_filter.html) function or a comprehension list along with the `apply` function. \n",
    "\n",
    ">- Write a function that removes `None` elements from a list\n",
    "- apply that function to the \"Bio_list\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function that removes `None` elements from a list\n",
    "def remove(list_to_clean, element_to_remove=[None, \"\"]):\n",
    "    # TODO: your code goes here\n",
    "    pass\n",
    "\n",
    "# TODO: apply that function to the \"Bio_list\" columns\n",
    "nobel_merged_df[\"Bio_list\"] =  # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the [`nltk`](http://www.nltk.org/) library to help us clean this data. Be sure to install the library with `pip` or `conda`.\n",
    "\n",
    "Use the `nltk.download('stopwords')` function to download the stopwords corpus. A you can see, the stopwords are common english words that do not carry significant information of a specific text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to capture in each bag of words, the words that are characteristic of a specific physicist. There are many words in the english language that are not useful for that. We call those words the stopwords.\n",
    "\n",
    "> Use your `remove` function to remove those words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words_to_remove = set(stopwords.words('english'))\n",
    "\n",
    "# TODO: remove the stop words\n",
    "nobel_merged_df[\"Bio_list\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Bio_list\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Write a function that removes the words that have only one character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: write a function that removes the words that have only one character\n",
    "def remove_one(list_to_clean):\n",
    "    pass\n",
    "\n",
    "# TODO: apply this function to the \"bio_list\" column\n",
    "nobel_merged_df[\"Bio_list\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Bio_list\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to remove all the words that appear too few times. This is an attempt to filter the words that are not relevant to the particular physics at play\n",
    "\n",
    "> Write a function that removes all the words under a certain amount of occurance. I would say that it is job to choose the threshold (if any!) on the number of occurance that you feel gives you satisfying results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function that remove all the words under a certain amount of occurance\n",
    "def remove_n_occurrance(list_to_clean, n = 1):\n",
    "    pass \n",
    " \n",
    "# TODO: apply this function to the \"bio_list\" column\n",
    "nobel_merged_df[\"Bio_list\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Bio_list\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are just going to keep each word once and remove the duplicate. You can use the function `set` to do so\n",
    "\n",
    "> Write a function that remove the duplicated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write a function that keeps only each element of a list only once\n",
    "def remove_duplicates(list_to_clean):\n",
    "    pass\n",
    "\n",
    "# TODO: apply this function to the \"bio_list\" column\n",
    "nobel_merged_df[\"Bio_list\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Bio_list\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now are going to try to guess from this data, what type of physics those physicists are practicing. [Wikipedia](https://en.wikipedia.org/wiki/Physics) identifies 6 types of physics (to be debated!): [Nuclear physics](https://en.wikipedia.org/wiki/Nuclear_physics), [particle physics](https://en.wikipedia.org/wiki/Particle_physics), [Atomic, molecular, and optical physics](https://en.wikipedia.org/wiki/Atomic,_molecular,_and_optical_physics), [Condensed matter physics](https://en.wikipedia.org/wiki/Condensed_matter_physics), [Astrophysics](https://en.wikipedia.org/wiki/Astrophysics) and [Physical_cosmology](https://en.wikipedia.org/wiki/Physical_cosmology). We are going to get the text data from those pages and look at the set of words that are 2 different pages.\n",
    "\n",
    "> Use your previously written functions to clean those the content from those Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_df = pd.DataFrame({\"Field\": [\"Nuclear physics\",\n",
    "                                     \"Particle physics\", \n",
    "                                     \"Atomic, molecular, and optical physics\", \n",
    "                                     \"Condensed matter physics\", \n",
    "                                     \"Astrophysics\",\n",
    "                                     \"Physical_cosmology\"],\n",
    "                           \"link\": [\"/wiki/Nuclear_physics\",\n",
    "                                     \"/wiki/Particle_physics\", \n",
    "                                     \"/wiki/Atomic,_molecular,_and_optical_physics\", \n",
    "                                     \"/wiki/Condensed_matter_physics\", \n",
    "                                     \"/wiki/Astrophysics\",\n",
    "                                     \"/wiki/Physical_cosmology\"]})\n",
    "\n",
    "physics_df.set_index(\"Field\", inplace=True)\n",
    "\n",
    "# TODO: gather and clean the data related to those physics fields wikipedia pages\n",
    "physics_df[\"Text_data\"] =  # YOUR CODE\n",
    "\n",
    "physics_df[\"Text_data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For each physicist compute the number of words in his biography that intersect with the physics fields pages. You can use the function [`intersection`](https://docs.python.org/2/library/sets.html) of the `set` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "list1 = [1, 2, 3, 4]\n",
    "list2 = [3, 4, 5, 6]\n",
    "set(list1).intersection(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function that count the number of words that intersect between two lists\n",
    "def intesect_count(list1, list2):\n",
    "    pass\n",
    "\n",
    "# TODO: create those columns\n",
    "nobel_merged_df[\"Count_intersect_Nuclear\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_intersect_Particle\"] =   # YOUR CODE\n",
    "nobel_merged_df[\"Count_intersect_Atomic\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_intersect_Condensed\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_intersect_Astrophysics\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_intersect_Cosmology\"] =  # YOUR CODE\n",
    "\n",
    "nobel_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For each physicist compute the total number of words contained in his biography and in each of the physics fields pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function that count the total number of unique words contained in two lists\n",
    "def total_count(list1, list2):\n",
    "    pass\n",
    "\n",
    "# TODO: create those columns\n",
    "nobel_merged_df[\"Count_total_Nuclear\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_total_Particle\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_total_Atomic\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_total_Condensed\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_total_Astrophysics\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Count_total_Cosmology\"] =  # YOUR CODE\n",
    "\n",
    "nobel_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to estimate the probability for words to belong to the wikipedia page of a physicist and to a physics field page using the following approximation:\n",
    "\\begin{equation}\n",
    "p(\\mbox{Same words for physicist P and field F}) \\simeq \\frac{\\mbox{Number of words in P and in F}}{\\mbox{Total number of words contained in P and F}}= \\frac{P\\cap F}{P\\cup F}\n",
    "\\end{equation} \n",
    "This \"probability\" is known as the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index).\n",
    "\n",
    ">For each physicist, compute the Jaccard index for words to be in both the physicist page and each physics field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Compute those columns\n",
    "nobel_merged_df[\"Proba_Nuclear\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Proba_Particle\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Proba_Atomic\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Proba_Condensed\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Proba_Astrophysics\"] =  # YOUR CODE\n",
    "nobel_merged_df[\"Proba_Cosmology\"] =  # YOUR CODE\n",
    "\n",
    "nobel_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Use the [`pd.idxmax`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html) to capture what field has the highest probability of intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proba_cols = [\"Proba_Nuclear\",\n",
    "              \"Proba_Particle\",\n",
    "              \"Proba_Atomic\",\n",
    "              \"Proba_Condensed\",\n",
    "              \"Proba_Astrophysics\",\n",
    "              \"Proba_Cosmology\"]\n",
    "\n",
    "# We normalize the probability to 1\n",
    "nobel_merged_df[proba_cols] = nobel_merged_df[proba_cols].apply(lambda x: x / sum(x), 1)\n",
    "\n",
    "# TODO: Which field each physicist belongs to? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Do you agree with this classification? How could we improve the analysis to get better classification?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
