{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6: Face Recognition\n",
    "\n",
    "We are going to extract the relevant information from face images to label images correctly.  We are going to create a data set of faces to train a classifier that can recognize specific people. In this homework we are going to make some use of [OpenCV](https://opencv.org/) so you need to download it:\n",
    "\n",
    "```\n",
    "source activate [YOUR ENVIREMENT]\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "You can import it by using the command:\n",
    "\n",
    "```\n",
    "import cv2\n",
    "```\n",
    "\n",
    "OpenCV is a very advanced library for Computer Vision applications.\n",
    "\n",
    "## 1. Create the data\n",
    "\n",
    "For this homework, I used some data available [here](http://www.cs.toronto.edu/~guerzhoy/320/proj3/faces_subset.txt) that is a subset of the much bigger [FaceScrub](http://vintage.winklerbros.net/facescrub.html) data set. I made available the `faces_subset.txt` in the current folder. This file contains actors' names, URLs to find pictures of those actors, numbers lists to help cropping the images to capture the faces only and other unnecessary information. We are going to use this file to download the images to then crop them.\n",
    "\n",
    "> - Load the `faces_subset.txt` file into a data frame. You can use the pandas [`read_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html) function. You can use the `header`, `usecols` and `names` arguments to load the relevant columns and to name them.\n",
    "- You will need to create 2 folders: \"uncropped\" and \"cropped\" in the current directory. We are going to use those folders to store the images.\n",
    "- We will download the images and store locally them with a unique file name in the folders \"uncropped\" and \"cropped\". Before downloading, create file names for each those images in a new column of the faces_subset data frame. In faces_subset, each row corresponds to different images of a limited amount of actors. The file names should capture the actor names, the image index (an integer) and the file format extension. For example, the row is:\n",
    "```\n",
    "Aaron Eckhart http://upload.wikimedia.org/wikipedia/commons/5/5d/AaronEckhart10TIFF.jpg\n",
    "```\n",
    "so a good file name could be:  `Aaron_Eckhart1.jpg`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## TODO: Load the faces_subset.txt with the relevant columns and name the columns\n",
    "\n",
    "## TODO: Create in a new column a file name each of the row that is going to be used to store the images locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use the following function to download all the images into the \"uncropped\" folder. You just need to populate the first argument `args` with a tuple `(URL, DESTINATION_FILE_NAME)` for each row. `URL` is the URL of the image and `DESTINATION_FILE_NAME` is \"`uncropped/FILE_NAME`\" where `FILE_NAME` is the file name you created to store the images locally. \n",
    "\n",
    "It might take some time and it will require about 400 MB of disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import threading\n",
    "\n",
    "def timeout(args=(), kwargs={}, timeout_duration=30, default=None):\n",
    "    '''From:\n",
    "    http://code.activestate.com/recipes/473878-timeout-function-using-threading/'''\n",
    "    \n",
    "    class InterruptableThread(threading.Thread):\n",
    "        def __init__(self):\n",
    "            threading.Thread.__init__(self)\n",
    "            self.result = None\n",
    "\n",
    "        def run(self):\n",
    "            try:\n",
    "                urllib.request.urlretrieve(*args)\n",
    "            except:\n",
    "                self.result = default\n",
    "\n",
    "    it = InterruptableThread()\n",
    "    it.start()\n",
    "    it.join(timeout_duration)\n",
    "      \n",
    "    if it.isAlive():\n",
    "        return False\n",
    "    else:\n",
    "        return it.result\n",
    "    \n",
    "## TODO: iterate through the rows and apply the timeout function to download the images into the \"uncropped\" folder   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to crop those images to capture only the faces of the actors. Included in `faces_subset.txt`, for each row, there is a list of 4 integers `x1, y1, x2, y2` that can be used to crop the images such that `img[y1:y2, x1:x2]` will center the image on the face. For each line in `faces_subset.txt`, you will need to load the image from the \"uncropped\" folder in gray scale, crop the image to the face, resize it to a $64 \\times 64$ shape and write the image back to the \"cropped\" folder. Write the `crop` function to achieve this:\n",
    "\n",
    ">- Load the image using the [`cv2.imread`](https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html?highlight=imread#imread) function with the `flags` argument being 0 to return the gray scale version of the image.\n",
    "- Parse the list of integers and crop each image.\n",
    "- Use the [`cv2.resize`](https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html) function to reshape the cropped image to a $64 \\times 64$ image.\n",
    "- Use the [`cv2.imwrite`](https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#imwrite) to write the image to the \"cropped\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: write the crop function\n",
    "## I use a try-except method to return None if the image does not exist in the \"uncropped\" folder\n",
    "def crop(filename, param):\n",
    "    try:\n",
    "        ## TODO: load the image\n",
    "        \n",
    "        ## TODO: the list of integers and crop the image\n",
    "        \n",
    "        ## TODO: resize the image to (64, 64)\n",
    "        \n",
    "        ## TODO: save the image to the \"cropped\" folder\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "## TODO: apply the function crop to each row in faces_subset.txt     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! We need now to load all images into a data frame to be able them as data. We are going to write 3 functions to do so. The first function `normalize` will convert an image to float normalize the image such that the maximum value is 1 and the minimum value is 0. The function `load_image` will load the image, normalize it, flatten it into a 1-dimentional array and return this array as a pandas Series. The last function `load_image_flip` will do the same thing than `load_image` but will flip the image around the y-axis to augment the data with the flipped version of each image. We will apply those functions to images of the \"cropped\" folder to create a data frame where each row correspond to an image and each column corresponds to a pixel. The data frame should contain twice as many rows as there are images since we augmented the data with the flipped version of each image. Finally we will copy the column containing the names in the `faces_subset.txt` to this new data frame.\n",
    "\n",
    ">- Write the function `normalize` that convert an image to float array and rescale the values such that the maximum value is 1 and the minimum value is 0.\n",
    "- Write the `load_image` function that loads an image from the \"cropped\" folder in gray scale, normalize it using the `normalize` function and flatten it with the [np.flatten](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.flatten.html) function. Cast the resulting array as a pandas series and return it.\n",
    "- Write the `load_image_flip` does the same thing than `load_image` but this time on the flipped image. You can use the [`np.fliplr`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.fliplr.html) to flip an image.\n",
    "- Apply those functions to create a data frame where each row corresponds to an image (2 rows for each image in the \"cropped\" folder) and each column corresponds to a pixel\n",
    "- Create a new column in this data frame containing the name of the actor related to the image\n",
    "\n",
    "The resulting data frame should look something like:\n",
    "\n",
    "<img src=\"images/all_faces.png\",width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: write the normalize function\n",
    "def normalize(im):\n",
    "    raise NotImplementedError\n",
    "\n",
    "## TODO: write the load_image function\n",
    "def load_image(filename):\n",
    "    try:\n",
    "        raise NotImplementedError        \n",
    "    except:\n",
    "        return pd.Series()\n",
    "\n",
    "## TODO: write the load_image_flip function\n",
    "def load_image_flip(filename):\n",
    "    try:\n",
    "        raise NotImplementedError      \n",
    "    except:\n",
    "        return pd.Series()\n",
    "\n",
    "## TODO: create a new data frame where you store all the images in a flat and normalized format\n",
    "## TODO: create a new column in this data frame to store the names of each actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Average those rows together and reshape the resulting array as $64 \\times 64$ array using the [np.reshape](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html) function. \n",
    "- Plot the \"mean face\" using the matplotlib imshow function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## TODO: Plot the average face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Learning the Faces\n",
    "\n",
    "### 2.1 Simple approach\n",
    "\n",
    "We are going now to try to train the best classifier on this data. For simplicity we going to limit ourselves to the Random Forest algorithm ([1](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm), [2](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)). Random Forest is available on scikit-learn [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). \n",
    "\n",
    ">- You will first remove the rows with missing values\n",
    "- Use the [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) to encode the column containing the names. \n",
    "- Fit Random Forests for `n_estimators=100` and for different values of the Random Forest's `max_features` argument between 1 and N (the total number of features) and plot those accuracies as a function of `max_features`. The imputs will be a matrix of flatten images and the target will be encoded version of the names. Random Forest by construct, allows to estimate the generalization error without crossvalidating. If you set the argument `oob_score=True`, the `oob_score_` attribute will be the estimation of accuracy. This is due to RF using the [Bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating) (Bootstrap aggregating) process. At training time, RF creates boostrap samples and not all the original samples belong the data used by RF. You can prove that only ~63.2% of the original data for each tree is used, leaving ~26.8% of data to estimate the performance of each tree. This remaining data is called the out of bag (OOB) data and the performance if called the out of bag performance. Do not forget `n_jobs=-1`.\n",
    "- Recompute the accuracy for the `max_features` value that yields that best performance but this time set `n_estimators=1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "## TODO: drop the rows with missing values\n",
    "\n",
    "## TODO: create the inputs and encode the target\n",
    "\n",
    "## TODO: Create different values of max_features\n",
    "Ms = # YOUR CODE\n",
    "\n",
    "for m in Ms:\n",
    "    ## TODO: Compute the accuracy for different values of max_features\n",
    "        \n",
    "## TODO: Plot the accuracy as a function of max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Recompute the accuracy for the max_features value that yields that best \n",
    "## performance but this time set n_estimators=1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature transformation: Scaling / PCA\n",
    "\n",
    "You may have noticed that the computation is quite slow and the performance is not great. Let's try to improve by transforming the data into the principal component basis. We are going to apply a [PCA](http://cs229.stanford.edu/notes/cs229-notes10.pdf) transformation for different number of principal components and we are going to train a Random Forest. We are going to compare the performance to the same algorithms but time after standardizing the data. We are going to use the [`Pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) process to stack different computational steps one after the others. We cannot use anymore the Out of Bag accuracy from Random Forest as the PCA and scaling transformations are part of the process (there would be some insampling problems). \n",
    "\n",
    ">- Create few values for `n_components` between 1 and 500. You will iterate though those values to select different numbers of principal components to use in training. \n",
    "- For each of those `n_components`, create 2 pipelines: one of [`StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), `PCA` and Random Forest and one with `PCA` and Random Forest only.\n",
    "- Use the [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to compute the [10-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics%29#k-fold_cross-validation) accuracy for stratified folds for different values of `n_components` argument and plot those accuracies as a function of `n_components` on the same graph. You can use the accuracy score by using \"accuracy\" for the `scoring` argument. You can use the [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) class to perform the stratified cross validation. Set the Random Forest's `n_estimators` to 1000, `n_jobs=-1` and leave the rest at the default value. Set PCA's `whiten=True`. Set StratifiedKFold's `shuffle=True`, `random_state=42`.\n",
    "- Compare your best result to the previous section's best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "## TODO: Create different values of n_components\n",
    "Ns = # YOUR CODE\n",
    "\n",
    "for n in Ns:\n",
    "    ## TODO: Set a pipeline of PCA and Random forest and another of StandardScaler, PCA and Random forest\n",
    "    \n",
    "    ## TODO: Compute the 10 fold-CV accuracy with stratified sampling for different values of n_components \n",
    "        \n",
    "## TODO: Plot the accuracy as a function of n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature selection\n",
    "\n",
    "It is quite possible that the first principal components are not actually the most optimal features to choose for Random Forest. We are going to select this time the best features by looking at the `feature_importances_` attribute of Random Forest. This time it is difficult to use the Pipeline along with the cross_val_score function therefore we are going to write our own crossvalidation process.\n",
    "\n",
    ">- We are going to use `StratifiedKFold` to split the data into 10 folds. Apply the `get_n_splits` function onto the inputs and the target. Use the `split` function to iterate thought the folds and split the data into train and test sets. Use `random_state=42` (the same than the previous section).\n",
    "- For each fold, create a pipeline of `StandardScaler` and `PCA` with `n_components=500`, `whiten=True`. For each fold fit this pipeline with the train set and transform the train and test sets. This step is to ensure that we have enough principal components to evaluate. \n",
    "- Perform a first Random Forest fit and capture the feature importance using the Random Forest's `feature_importances_` attribute. \n",
    "- Perform a second Random Forest fit but this time, only using the $n$ best features where $n$ was the number for `n_components` in the previous section (2.2) that yielded the best performance.\n",
    "- For each fold, measure the accuracy on the test set.\n",
    "- Average the results for the all the folds and compare the performance with the previous section\n",
    "\n",
    "Your classification performance should have drastically improved from the simple approach!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: We are going to use StratifiedKFold to split the data into 10 folds\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42)\n",
    "## TODO: Apply the get_n_splits function onto the inputs and the target. \n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "## TODO: Use the split function to iterate thought the folds.\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    \n",
    "    ## TODO: split the data into train and test sets\n",
    "    \n",
    "    ## TODO: create a pipeline of StandardScaler and PCA with n_components=500, whiten=True\n",
    "     \n",
    "    ## TODO: fit this pipeline with the train set and transform the train and test sets. \n",
    "      \n",
    "    ## TODO: Perform a first Random Forest fit \n",
    "    \n",
    "    ## TODO: capture the feature importance using the Random Forest's feature_importances_ attribute\n",
    "       \n",
    "    ## TODO: Perform a second Random Forest fit with the best features from feature_importances_ \n",
    "    \n",
    "    ## TODO: measure the accuracy on the test set\n",
    "\n",
    "## TODO: Average the results for the all the folds and compare the performance with the previous section "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis for Face recognition\n",
    "\n",
    "We are going to use PCA to recognize if an image if the one of a human face or the something else\n",
    "\n",
    "### 3.1 The eigen faces\n",
    "\n",
    "First of all, let's use the previous data set to visualize the eigen vectors coming out of the singular value decomposition done in PCA.\n",
    "\n",
    ">- Fit a PCA for `n_components=50`. You can try with and without scaling.\n",
    "- Plot the 10 first eigen faces by using the `components_` attribute from PCA. You will need to reshape the vectors in a $64\\times 64$ array. To plot the images in gray scale and one after the other you can use:\n",
    "```\n",
    "plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: Plot the 10 first eigen faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Image reconstruction\n",
    "\n",
    "We are now going to project images onto the principal component basis learned with our original data set and reconstruct the images in the original basis. Only images that are well described by the eigen faces basis should be well reconstructed. \n",
    "\n",
    ">- Fit PCA to the original data with a `n_components` argument of your choice. \n",
    "- For each image in the \"additional_images/test_face_recognition\" folder use the `transform` and `inverse_transform` functions to project the image into the PCA basis and then reconstruct the image into the original basis. You can try with and without scaling. For each image, you will need to apply the same transformations you have done to the original data.\n",
    "- For each original and reconstructed image compute the $RMSE=\\sqrt{\\overline{(original - reconstructed)^2}} $\n",
    "- Plot each image and its reconstructed version.\n",
    "- Can you find a `n_components` value and a threshold on RMSE such that you can classify an image as face or not?\n",
    "\n",
    "This following script will give you every files within the folder \n",
    "```\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "folder = \"./additional_images/test_face_recognition/\"\n",
    "\n",
    "all_files = os.listdir(folder)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "folder = \"./additional_images/test_face_recognition/\"\n",
    "\n",
    "all_files = os.listdir(folder)\n",
    "\n",
    "## TODO: choose n_components\n",
    "\n",
    "## TODO: for every image project and reconstruct the image\n",
    "## TODO: Plot each image and it reconstructed version\n",
    "for f in all_files:\n",
    "    ## TODO: Plot each image and it reconstructed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning gender\n",
    "\n",
    "### 4.1 Clustering approach\n",
    "\n",
    "Often clustering methods can help supervised learning algorithms learning to create new features. To highlight that we are going to are going to cluster the original data into 2 clusters and compare the results with the genders. You may have noticed that in the original data, the first actors are male and the last ones are female.  \n",
    "\n",
    ">- Build a gender binary target\n",
    "- Try [`MiniBatchKMeans`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html), [`GaussianMixture`](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) and [`AgglomerativeClustering`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) to cluster the data into 2 clusters. You can use PCA and/or scaling if you find it improves the performance.\n",
    "- Compare the clusters with the gender target and try to find the combination PCA / scaling / feature selection and the different arguments of the different functions that yields the best clustering of the genders.  \n",
    "\n",
    "Even if does yield performance anywhere close to supervised learning methods, the learning done through those methods can improve the performance of a supervised learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: cluster the data into 2 clusters and compare them with the gender target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Supervised learning Approach\n",
    "\n",
    "We are going to test our trained models onto very different images to see if they generalize well.\n",
    ">- Create a pipeline of StandardScaler, PCA and Random Forest (choose good parameters) and fit this pipeline with the full original data and the gender target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: train a pipeline to learn gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to use apply this trained model to the images in the \"additional_images/test_gender_recognition\" folder. To do so we are to use the [Haar-cascade Detection from opencv](https://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html). The following code will allow us to capture only the faces:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# We first load the trained AdaBoost model from opencv to recognize face\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# We load one of our images\n",
    "img = cv2.imread('./additional_images/test_gender_recognition/image1.jpg')\n",
    "# We convert it to gray scale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# We detect the faces on the image\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "# For each image we plot a rectangle around the face\n",
    "for x, y, w, h in faces:\n",
    "    # We plot the rectange\n",
    "    cv2.rectangle(img,(x, y),(x + w, y + h),(255,0,0),2)\n",
    "    # We write some text\n",
    "    cv2.putText(img, \"Picture\", (x,y), cv2.FONT_HERSHEY_SIMPLEX,  1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "# We plot the resulting image\n",
    "# Opencv reads images as BGR but matplotlib RGB images so we need to convert\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Use the previous script to label each face with a gender (\"Male\", \"Female\") as predicted by your trained model.\n",
    "- Try to tune your model to get the best performance for all the images in the \"additional_images/test_gender_recognition\" folder. \n",
    "- Explain what we could do to improve our predictions.\n",
    "\n",
    "Here how it could look like\n",
    "<img src=\"images/example.png\",width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Label the faces for each image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
