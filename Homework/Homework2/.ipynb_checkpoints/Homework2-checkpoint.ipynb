{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Social Network Data Visualisation\n",
    "\n",
    "- [Physics as network](#Physics as network)\n",
    "- [Getting the Data](#Getting the Data)\n",
    "- [Cleaning the data](#Cleaning the data)\n",
    "- [Compiling the nodes data](#Compiling the nodes data)\n",
    "- [Compiling the links data](#Compiling the links data)\n",
    "\n",
    "In this homework, we are going to reuse the data set of the previous homework to look at it from a Social Network prospective. A social network is a construct where social actors can be represented as nodes linked by edges on a graph. As an example, here is the Facebook social network\n",
    "\n",
    "<img src=\"additional/facebook_world_friend_map.png\" width=600>\n",
    "\n",
    "We are using the [d3.js](https://d3js.org/) javascript library, a data visualization library, to represent our networks. You can see [here](https://bl.ocks.org/mbostock/4062045) an example of such a network.\n",
    "\n",
    "\n",
    "## Physics as a social network  <a class=\"anchor\" id=\"Physics as network\"></a>\n",
    "\n",
    "A social network can be represented as a graph with a set of nodes and a set of links between those nodes:\n",
    "\n",
    "<img src=\"additional/small_undirected_network_labeled.png\" width=300>\n",
    "\n",
    "In the previous homework, we used data sets with physicists and physics domains. If we consider each physicist and each physics domain as a possible node on a network, the problem becomes building edges between those. Here an example of how to represent graphs with python data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_network = {\n",
    "    \"nodes\": [\n",
    "        {\"id\": \"Albert Einstein\"},\n",
    "        {\"id\": \"Paul Dirac\"},\n",
    "        {\"id\": \"Niels Bohr\"}\n",
    "    ],\n",
    "    \"links\": [\n",
    "        {\"source\": \"Albert Einstein\", \"target\": \"Paul Dirac\"},\n",
    "        {\"source\": \"Albert Einstein\", \"target\": \"Niels Bohr\"},\n",
    "        {\"source\": \"Paul Dirac\", \"target\": \"Niels Bohr\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "## We dump this network into a .json file\n",
    "import json\n",
    "with open(\"./data/small_network.json\",\"w\") as f:\n",
    "    json.dump(small_network, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.9'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a mac, the following script is going to open a safari window to visualize this network, otherwise you can just open the `small_network.html` file with Safari or Firefox. For some reason, it does not work with chrome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"open -a /Applications/Safari.app ./small_network.html --args --allow-file-access-from-files\")\n",
    "# os.system(\"open -a /Applications/Google\\ Chrome.app ./small_network.html --args --allow-file-access-from-files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, that is not a very interesting network and we are going to build a more substantial one! D3.js expects the data in a specific format as shown above and we are going to shape our data to follow those requirements. For each node, we need an \"id\" tag and we can add other attribute if we desire. For the links, we need a \"source\" and a \"target\" to connect nodes and we can add other attributes also. \n",
    "\n",
    "In the following network, we are going to add the \"length\" attribute that captures how big is each node and the \"value\" attribute for each link that captures how \"strong\" the links are. We also are going to distinguish between 2 types of nodes: the nodes for the physicist and the nodes for the physics domains. Those 2 sets of nodes will be distinguished by the attribute \"group\". For example:\n",
    "\n",
    "```\n",
    "small_network = {\n",
    "    \"nodes\": [\n",
    "        {\"id\": \"Albert Einstein\", \"group\": 1, \"length\": 100},\n",
    "        {\"id\": \"Paul Dirac\", \"group\": 1, \"length\": 200},\n",
    "        {\"id\": \"Niels Bohr\", \"group\": 1, \"length\": 300}\n",
    "    ],\n",
    "    \"links\": [\n",
    "        {\"source\": \"Albert Einstein\", \"target\": \"Paul Dirac\", \"value\": 0.5},\n",
    "        {\"source\": \"Albert Einstein\", \"target\": \"Niels Bohr\", \"value\": 0.4},\n",
    "        {\"source\": \"Paul Dirac\", \"target\": \"Niels Bohr\", \"value\": 0.3}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Getting the Data <a class=\"anchor\" id=\"Getting the Data\"></a>\n",
    "\n",
    "We first going to gather the data needed for this project. We are going to extract the words in each Wikipedia page to understand the relation between each physicist and physics domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## We get the nobel data set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from httplib2 import Http\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    def __init__(self, url):  \n",
    "        http = Http()\n",
    "        status, response = http.request(url)\n",
    "        tables = BeautifulSoup(response, \"lxml\", \n",
    "                              parse_only=SoupStrainer(\"table\", {\"class\":\"wikitable sortable\"}))\n",
    "        self.table = tables.contents[1]\n",
    "    \n",
    "    def parse_table(self):      \n",
    "        rows = self.table.find_all(\"tr\")\n",
    "        header = self.parse_header(rows[0])\n",
    "        table_array = [self.parse_row(row) for row in rows[1:]]\n",
    "        table_df = pd.DataFrame(table_array, columns=header).apply(self.clean_table, 1)\n",
    "        return table_df.replace({\"Year\":{'':np.nan}})\n",
    "        \n",
    "    def parse_row(self, row):     \n",
    "        columns = row.find_all(\"td\")\n",
    "        return [BeautifulSoup.get_text(col).strip() for col in columns if BeautifulSoup.get_text(col) != '']\n",
    "    \n",
    "    def parse_header(self, row):     \n",
    "        columns = row.find_all(\"th\")\n",
    "        return [BeautifulSoup.get_text(col).strip() for col in columns if BeautifulSoup.get_text(col) != \"\"]\n",
    "    \n",
    "    def clean_table(self, row):\n",
    "        if not row.iloc[0].isdigit() and row.iloc[0] != '':\n",
    "            return row.shift(1)\n",
    "        else:\n",
    "            return row\n",
    "        \n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Physics\"        \n",
    "parser = Parser(url)   \n",
    "nobel_df = parser.parse_table()\n",
    "nobel_df.columns = [\"Year\", \"Laureate\", \"Country\", \"Rationale\"]\n",
    "nobel_df.dropna(subset=[\"Country\"], inplace=True)\n",
    "nobel_df.fillna(method=\"ffill\", inplace=True)\n",
    "nobel_df.drop([\"Year\", \"Country\", \"Rationale\"], 1, inplace=True)\n",
    "\n",
    "http = Http()\n",
    "status, response = http.request(url)\n",
    "\n",
    "table = BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer('table'))\n",
    "link_df = pd.DataFrame([[x.string, x[\"href\"]] for x in table.contents[1].find_all(\"a\")],\n",
    "                       columns=[\"Laureate\", \"link\"]).drop_duplicates()\n",
    "\n",
    "nobel_df = nobel_df.merge(link_df, on=\"Laureate\", how=\"left\")\n",
    "nobel_df.set_index(\"Laureate\", inplace=True)\n",
    "nobel_df.drop_duplicates(inplace=True)\n",
    "nobel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also going to extract the links of each of the physics domains listed in the Research fields table of the [https://en.wikipedia.org/wiki/Physics](https://en.wikipedia.org/wiki/Physics) Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## We get the physics links\n",
    "url = \"https://en.wikipedia.org/wiki/Physics\"\n",
    "\n",
    "http = Http()\n",
    "status, response = http.request(url)\n",
    "\n",
    "table = BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer('table'))\n",
    "physics_df = pd.DataFrame([[x.string.lower(), x[\"href\"].lower()] for x in table.contents[2].find_all(\"a\")],\n",
    "                       columns=[\"Physics_domain\", \"link\"]).drop_duplicates()\n",
    "\n",
    "physics_df = physics_df.groupby(\"Physics_domain\").first()\n",
    "physics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cleaning the data <a class=\"anchor\" id=\"Cleaning the data\"></a>\n",
    "\n",
    ">Use the code from your previous homework to create the different functions to clean the get the word data and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "## We get the bios\n",
    "def get_text(link, root_website = \"https://en.wikipedia.org\"):    \n",
    "    http = Http()\n",
    "    status, response = http.request(root_website + link)\n",
    "    body = BeautifulSoup(response, \"lxml\", parse_only=SoupStrainer(\"div\", {\"id\":\"mw-content-text\"}))\n",
    "    return BeautifulSoup.get_text(body.contents[1])\n",
    "\n",
    "# TODO: copy your clean_string function from the previous homework\n",
    "def clean_string(string):\n",
    "    pass\n",
    "\n",
    "# TODO: copy your remove function from the previous homework\n",
    "def remove(list_to_clean, element_to_remove=[None, \"\"]):\n",
    "    pass\n",
    "\n",
    "# TODO: copy your remove_one function from the previous homework\n",
    "def remove_one(list_to_clean):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now going to write a function that takes a data frame with a \"link\" column and return a column of list of words. We basically aggregate all the above function into one to reproduce what was done in the previous homework. Note that here we DO NOT use the function that keep only a unique element of each list nor the one that filter on the number of occurance.\n",
    "\n",
    "> Write a function that applies all the previous functions to clean a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words_to_remove = set(stopwords.words('english'))\n",
    "\n",
    "# TODO: aggregate all the above function into one to return a list of words from each link\n",
    "def clean_everything(df):\n",
    "    pass\n",
    "\n",
    "physics_df[\"physics_list\"] = clean_everything(physics_df)\n",
    "nobel_df[\"physics_list\"] = clean_everything(nobel_df)\n",
    "nobel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw last time that there are many words that are not relevant to physics concepts in those Wikipedia pages. We are going to attempt to filter those with the simple following approach. \n",
    "\n",
    "We are going to compile a set of all the unique words in the `nobel_df` lists and a set of all the unique words in the `physics_df` lists. By taking the intersection of those 2 sets, we can subset the words corpus to something more relevant to physics.\n",
    "\n",
    ">- Compile a set of all unique words in `nobel_df[\"physics_list\"]`. You can use the function [`pd.sum`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html) to concatenate lists. You can cast the final list to a [`set`](https://docs.python.org/2/library/sets.html)\n",
    "- Compile a set of all unique words in `physics_df[\"physics_list\"]`.\n",
    "- Compile the intersection of those 2 sets using the `intersection` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: find all the words in nobel_df[\"physics_list\"]\n",
    "all_nobel_words =  # YOUR CODE\n",
    "\n",
    "# TODO: find all the words in physics_df[\"physics_list\"]\n",
    "all_physics_words =  # YOUR CODE\n",
    "\n",
    "# TODO: find all the intersection of all_nobel_words and all_physics_words\n",
    "physics_corpus =  # YOUR CODE\n",
    "\n",
    "physics_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(physics_corpus), len(all_nobel_words), len(all_physics_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Write a function that keep only specific words from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: write a function that keep only specific words from a list\n",
    "def keep_only(list_to_clean, corpus=physics_corpus):\n",
    "    pass\n",
    "    \n",
    "nobel_df[\"physics_list_clean\"] = nobel_df[\"physics_list\"].apply(keep_only)\n",
    "physics_df[\"physics_list_clean\"] = physics_df[\"physics_list\"].apply(keep_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Compiling the nodes data <a class=\"anchor\" id=\"Compiling the nodes data\"></a>\n",
    "\n",
    "For those 2 dataframes, we are going to create 2 additional columns:\n",
    "    \n",
    ">- create columns \"length\" that counts the number of words in each list. This column will be used to capture the size of the nodes in the networks. Basically we are going to say: the more words in the Wikipedia page, the more significant the physicist or physics domain is.\n",
    "- create columns \"group\" with a unique value for each of those dataframes. Set the value to 1 in the `nobel_df` dataframe and 0 for the `physics_df` dataframe. This columns will be used to distinguish the physicists from the physics domains and attribute them different colors in the network visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: compute the length of each list\n",
    "nobel_df[\"length\"] =  # YOUR CODE\n",
    "physics_df[\"length\"] =  # YOUR CODE\n",
    "\n",
    "# TODO: Set this column to 1\n",
    "nobel_df[\"group\"] =  # YOUR CODE\n",
    "# TODO: Set this column to 0\n",
    "physics_df[\"group\"] =  # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the those 2 dataframes into the `nodes_df` dataframe. \n",
    "\n",
    ">Use the [`pd.concat`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) function to do so and only keep the \"length\" and \"group\" columns. The concatenation needs to be done along the row axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: concatenate those two dataframe into the nodes_df dataframe. \n",
    "# keep only the \"length\" and \"group\" columns.\n",
    "nodes_df =  # YOUR CODE\n",
    "\n",
    "nodes_df.index.name = \"id\"\n",
    "nodes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this dataframe, we can easily format the data as a list of dictionaries as the d3.js library expects the data to be. We have the \"length\" attribute for the size of the node, the \"group\" attribute to distinguish between physicists and physics domains and each node has a unique \"id\" tag represented by the names.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_list = list(nodes_df.reset_index().transpose().to_dict().values())\n",
    "nodes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Compiling the links data <a class=\"anchor\" id=\"Compiling the links data\"></a>\n",
    "\n",
    "We have the nodes, we need to find a way to connect them. We are going to compute the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between each of the wikipedia pages. It is called the cosine similarity because a dot product between 2 vectors $\\mathbf{A}$ and $\\mathbf{B}$ can be express as:\n",
    "\\begin{equation}\n",
    "A\\cdot B = \\Vert A\\Vert_2\\Vert B\\Vert_2\\cos\\theta\n",
    "\\end{equation}\n",
    "where $\\theta$ is the angle between the 2 vectors. Similarly:\n",
    "\\begin{equation}\n",
    "\\cos\\theta = \\frac{ A\\cdot B}{\\Vert A\\Vert_2\\Vert B\\Vert_2}\n",
    "\\end{equation}\n",
    "$\\cos\\theta\\in[-1,1]$ and specifically $\\cos\\theta = 1$ if the 2 vectors are in the same direction or $\\cos\\theta = -1$ if the 2 vectors are in the opposite direction. The matter here becomes to be able to express a Wikipedia page as a vector. I suggest here a simple approach but there are many ways to achieve this. \n",
    "\n",
    "We defined earlier the corpus of physics words `physics_corpus`. Each word can be thought as a orthogonal basis defining a vector space where our Wikipedia pages are living in. Each component can be represented by the number of time a specific word appear in a page. As an example, imaging a page $P$ represented by the following list of words:\n",
    "```\n",
    "P_list = [\"data\", \"data\", \"science\", \"python\", \"python\"]\n",
    "```\n",
    "And let's imaging that we have a simple word corpus:\n",
    "```\n",
    "corpus = {\"engineering\", \"data\", \"science\", \"python\"}\n",
    "```\n",
    "Then in that basis $P$ could be represented by a vector \n",
    "\\begin{equation}\n",
    "\\mathbf{P}=\\left(\\begin{matrix}\n",
    "  0  \\\\\n",
    "  2  \\\\\n",
    "  1  \\\\\n",
    "  2\n",
    " \\end{matrix}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "We need to express each Wikipedia page as such vector. \n",
    "\n",
    ">Let's start by creating a dataframe with as columns, all the indices of the `nodes_df` dataframe (`nodes_df.index.values`) and as index, the whole `physics_corpus` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: create a data frame with the index of nodes_df as columns and physics_corpus as index\n",
    "words_vector =  # YOUR CODE\n",
    "words_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill this table we are going to use the [`value_counts`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html) function on the list of words contained in `nobel_df[\"physics_list_clean\"]` and `physics_df[\"physics_list_clean\"]`. \n",
    "\n",
    ">Write a function that takes a list and return a value_counts. The return value should be a pandas series with the words as index. We are using this function to populate the `words_vector` dataframe. Note that because `words_vector` already has an index, the values get populated at the right place automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: write a function that take a list and return the a word count\n",
    "def count_words(list_to_count):\n",
    "    pass\n",
    "\n",
    "words_vector.loc[:,nobel_df.index] = nobel_df[\"physics_list_clean\"].apply(count_words).transpose()\n",
    "words_vector.loc[:,physics_df.index] = physics_df[\"physics_list_clean\"].apply(count_words).transpose()\n",
    "words_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many entries in this dataframe that appear as `NaN`. We just need to replace those missing values by 0 since they indicate that no records of the word were found for those pages. \n",
    "\n",
    ">Use the function [`pd.fillna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) to fill with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: fill the missing values\n",
    "words_vector = # YOUR CODE\n",
    "words_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">- Write a function that takes 2 vectors (2 pandas series) and return the cosine similarity index. You can use the function [`dot`](http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.Series.dot.html), [`pow`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pow.html) and `sum` if you like.\n",
    "- Use this function to fill the `similarity_df` dataframe\n",
    "- Bonus points if you can compute this dataframe using matrix algebra ([`dot`](http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.DataFrame.dot.html)) without having to iterate through the columns. Hint create 2 dataframe: one that is the dot products of words_vector with itself and one that represent a matrix of norm products. Then divide one matrix by the other element wise. For this case, you would not need to use the `compute_similarity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: write a function that takes 2 vectors and return the cosine similarity index\n",
    "def compute_similarity(vect1, vect2):\n",
    "    return vect1.dot(vect2) / (np.sqrt(vect1.pow(2).sum()) * np.sqrt(vect2.pow(2).sum()))\n",
    "\n",
    "similarity_df = pd.DataFrame(columns=words_vector.columns, index=words_vector.columns, dtype=float)\n",
    "\n",
    "# TODO: fill the similarity_df dataframe with the cosine similarity\n",
    "\n",
    "# TODO: bonus points if you can compute this dataframe using matrix algebra \n",
    "\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to \"melt\" `similarity_df` into a long dataframe: \n",
    "\n",
    ">- We need to reset the index of `similarity_df` ([`pd.reset_index`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html))\n",
    "- Then we use the [`melt`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html) function to melt the dataframe. Use the resetted index as `id_vars`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: reset the index and melt the dataframe\n",
    "melted_df =  # YOUR CODE\n",
    "\n",
    "melted_df.columns = [\"source\", \"target\", \"value\"]\n",
    "melted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that at this point we have something close to what is necessary to create the links data. There are 3 things we need do to finalize our dataset. First, it is unnecessary to have \"source\" equal to the \"target\" (a node does not need to be linked to itself). Second, we have a duplicated links because in our case a \"source\" has the same role than a \"target\" and can be interchanged (our graph is not directed). Third, we need to subset the links set because there are too many for the program to run efficiently.\n",
    "\n",
    "Let's shuffle the data set rowwise ([`sample`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html)). This help us to not bias our links selection due to a prior alphabetic ordering of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "melted_df = melted_df.sample(frac=1.).reset_index(drop=True)\n",
    "melted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then going to find the pairs of (\"source\", \"target\") that are equal to pairs (\"target\", \"source\"). To do that we are going to merge `melted_df` with itself where (\"source\", \"target\") = (\"target\", \"source\").\n",
    "\n",
    ">- merge it with itself with `left_on=[\"source\", \"target\"]` and `right_on=[\"target\", \"source\"]`. Pass the dataframe with the index resetted using [`pd.reset_index`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: merge melted_df with itself\n",
    "merged_df =  # YOUR CODE\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can see that each pair of (\"source\", \"target\") has the redondant equivalent (\"target\", \"source\"). This also highlight the cases where \"source\" = \"target\". To filter the useless rows we can simply pick the (\"source\", \"target\") pair or the (\"target\", \"source\") pick to remove. Let's choose which pair to remove by capturing the index we want to remove\n",
    "\n",
    ">- Look at the pair of columns `merged_df[[\"index_x\", \"index_y\"]]` and simple choose the greater between the two using [`max`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.max.html). By selecting only the unique values ([`unique`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html)) of the resulting list of indices we have selected the index to remove and we can drop them using [`drop`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: find the index to drop\n",
    "index_to_drop =  # YOUR CODE\n",
    "# TODO: use the index_to_drop to subset the melted_df dataframe\n",
    "melted_df_sub =  # YOUR CODE\n",
    "melted_df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have filtered quite a bit of rows but it still is too many for the network simulation to run efficiently. For each source, we are going to select the 10 highest values. \n",
    "\n",
    ">- Group `melted_df_sub` by \"source\" using the [`groupby`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) method and select the 10 targets that have the highest values using the [`nlargest`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nlargest.html) method.\n",
    "- The resulting pandas Series has a multiindex with 2 levels. We need to get the level 1 of the multiindex to know which rows to keep in `melted_df_sub`. You can get it using the function [`get_level_values`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.get_level_values.html) on the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Group melted_df_sub by \"source\" using the groupby method and select the 10 \n",
    "# targets that have the highest values using the nlargest method\n",
    "largest_df =  # YOUR CODE\n",
    "# TODO: get the level 1 of the multiindex\n",
    "index_to_keep =  # YOUR CODE\n",
    "\n",
    "links_df = melted_df_sub.loc[index_to_keep]\n",
    "links_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cast this data frame as a list of dictionaries as we have done for the list of nodes. \n",
    "\n",
    ">Use a similar code to than for the nodes to create a list of links: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: create the list of links\n",
    "links_list =  # YOUR CODE\n",
    "links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the final dictionary for the network and save it into a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_dict = {\"nodes\": nodes_list,\n",
    "                \"links\": links_list}\n",
    "\n",
    "with open(\"./data/physicists.json\",\"w\") as f:\n",
    "    json.dump(network_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a mac, the following script is going to open a safari window to visualize this network, otherwise you can just open the index.html file with Safari or Firefox. For some reason it does not work with chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"open -a /Applications/Safari.app ./index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the parameters to try to find nodes that tend to be grouped together. You can try to recreate the network with with different number of links."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
